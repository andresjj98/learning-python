# -*- coding: utf-8 -*-
"""trabajo final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RWSkJW_hQFy0oa1RFtTwKi_BkSD-Uqbg
"""

# Importacion de las librerias
import pandas as pd
from google.colab import files
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score

#asignacion de los nombres de las columnas
datos = [
    "symboling", "normalized-losses", "make", "fuel-type", "aspiration", "num-of-doors", "body-style", "drive-wheels","engine-location", "wheel-base", "length", "width", "height", "curb-weight", "engine-type", "num-of-cylinders", "engine-size",    "fuel-system", "bore", "stroke", "compression-ratio", "horsepower", "peak-rpm", "city-mpg", "highway-mpg", "price"
]

#importacion del dataframe
df = pd.read_csv("/content/drive/MyDrive/inteligencia computacional/Trabajo final del diplomado/archivos/imports-85.data", names=datos, na_values="?")

#Visualizacion del dataframe
df

"""##Preparacion de los datos"""

# Cargar el conjunto de datos
data = df

# Análisis de correlación
correlation = data[['symboling', 'normalized-losses']].corr()

# Visualización
sns.scatterplot(x='symboling', y='normalized-losses', data=data)
plt.title('Relación entre Symboling y Normalized-losses')
plt.xlabel('Symboling')
plt.ylabel('Normalized-losses')
plt.show()

# Coeficiente de correlación
print("Coeficiente de correlación:")
print(correlation)

# Visualización de la distribución de "normalized-losses" utilizando un histograma
plt.figure(figsize=(8, 6))
sns.histplot(df['normalized-losses'], bins=20, kde=False, color='skyblue')
plt.title('Distribución de normalized-losses')
plt.xlabel('normalized-losses')
plt.ylabel('Frecuencia')
plt.show()

# Visualización de la distribución de "symboling" utilizando un histograma
plt.figure(figsize=(8, 6))
sns.histplot(df['symboling'], bins=6, kde=False, color='skyblue')
plt.title('Distribución de Symboling')
plt.xlabel('Symboling')
plt.ylabel('Frecuencia')
plt.show()

# Exploración de la relación entre "symboling" y "price" utilizando un gráfico de dispersión
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='symboling', y='price', color='salmon')
plt.title('Relación entre Symboling y Price')
plt.xlabel('Symboling')
plt.ylabel('Precio')
plt.show()

# Exploración de la relación entre "symboling" y "make" utilizando un boxplot
plt.figure(figsize=(12, 8))
sns.boxplot(data=df, x='make', y='symboling', palette='viridis')
plt.title('Relación entre Symboling y Make')
plt.xlabel('Marca')
plt.ylabel('symboling')
plt.xticks(rotation=90)
plt.show()

# Estadísticas descriptivas de "symboling"
print("Estadísticas descriptivas de Symboling:")
print(df['symboling'].describe())

# Calcular la media de Symboling
media = df['symboling'].mean()

# Calcular la mediana de Symboling
mediana = df['symboling'].median()

# Calcular la moda de Symboling
moda = df['symboling'].mode()[0]  # Puede haber más de una moda, por lo que se toma la primera (si existe)

# Calcular la desviación estándar de Symboling
desviacion_estandar = df['symboling'].std()

# Imprimir los resultados
print("Media de Symboling:", media)
print("Mediana de Symboling:", mediana)
print("Moda de Symboling:", moda)
print("Desviación Estándar de Symboling:", desviacion_estandar)

# Calcular la media de normalized-losses
media = df['normalized-losses'].mean()

# Calcular la mediana de normalized-losses
mediana = df['normalized-losses'].median()

# Calcular la moda de normalized-losses
moda = df['normalized-losses'].mode()[0]  # Puede haber más de una moda, por lo que se toma la primera (si existe)

# Calcular la desviación estándar de normalized-losses
desviacion_estandar = df['normalized-losses'].std()

# Imprimir los resultados
print("Media de normalized-losses:", media)
print("Mediana de normalized-losses:", mediana)
print("Moda de normalized-losses:", moda)
print("Desviación Estándar de normalized-losses:", desviacion_estandar)

"""##comparacion la realcion entre normalized-losses"""

# Visualización de la distribución de "normalized-losses" utilizando un histograma
plt.figure(figsize=(8, 6))
sns.histplot(df['normalized-losses'], bins=20, kde=False, color='skyblue')
plt.title('Distribución de normalized-losses')
plt.xlabel('normalized-losses')
plt.ylabel('Frecuencia')
plt.show()

# Visualización de la distribución de "symboling" utilizando un histograma
plt.figure(figsize=(8, 6))
sns.histplot(df['normalized-losses'], bins=6, kde=False, color='skyblue')
plt.title('Distribución de normalized-losses')
plt.xlabel('normalized-losses')
plt.ylabel('Frecuencia')
plt.show()

# Exploración de la relación entre "symboling" y "price" utilizando un gráfico de dispersión
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='normalized-losses', y='price', color='salmon')
plt.title('Relación entre normalized-losses y Price')
plt.xlabel('normalized-losses')
plt.ylabel('Precio')
plt.show()

# Exploración de la relación entre "symboling" y "make" utilizando un boxplot
plt.figure(figsize=(12, 8))
sns.boxplot(data=df, x='make', y='normalized-losses', palette='viridis')
plt.title('Relación entre normalized-losses y marca')
plt.xlabel('Marca')
plt.ylabel('normalized-losses')
plt.xticks(rotation=90)
plt.show()

# Estadísticas descriptivas de "symboling"
print("Estadísticas descriptivas de Symboling:")
print(df['normalized-losses'].describe())

"""## Analisis de valores nulos"""

N = len(df)  # Obtener el número total de filas en el DataFrame

for column in df:
    if column not in ["normalized-losses", "num-of-doors",'bore', 'stroke', 'horsepower', 'peak-rpm', 'price']:
        print(column)
        c = df[column].isnull().sum()  # Contar el número de valores nulos en la columna
        p = 1 - c / N  # Calcular la proporción de valores no nulos en la columna
        print(column, c, p * 100)
    else:
        print(column)
        c = df[column].isnull().sum()  # Contar el número de valores nulos en la columna
        p = 1 - c / N  # Calcular la proporción de valores no nulos en la columna
        print(column, c, p * 100)

# Hacer una copia del DataFrame
dfclean = df.copy()

# Crear un imputador para reemplazar valores faltantes con la media
imputer_num = SimpleImputer(strategy='mean')

# Seleccionar columnas numéricas con datos faltantes
data_mising = ['normalized-losses', 'bore', 'stroke', 'peak-rpm', 'horsepower', 'price']

# Imputar valores faltantes en columnas numéricas
dfclean[data_mising] = imputer_num.fit_transform(dfclean[data_mising])

# Imputación de datos faltantes para variables categóricas
# Crear un imputador para reemplazar valores faltantes con el valor más frecuente
cat_imp = SimpleImputer(strategy='most_frequent')

# Seleccionar columnas categóricas con datos faltantes
cat_mising = ['num-of-doors']

# Imputar valores faltantes en columnas categóricas
dfclean[cat_mising] = cat_imp.fit_transform(dfclean[cat_mising])

columns_to_drop = ['fuel-system', 'bore', 'stroke', 'peak-rpm']
dfclean.drop(columns_to_drop, axis=1, inplace=True)

N = len(dfclean)  # Obtener el número total de filas en el DataFrame

for column in dfclean:
    if column not in ["normalized-losses", "num-of-doors",'bore', 'stroke', 'horsepower', 'peak-rpm', 'price']:
        print(column)
        c = dfclean[column].isnull().sum()  # Contar el número de valores nulos en la columna
        p = 1 - c / N  # Calcular la proporción de valores no nulos en la columna
        print(column, c, p * 100)
    else:
        print(column)
        c = dfclean[column].isnull().sum()  # Contar el número de valores nulos en la columna
        p = 1 - c / N  # Calcular la proporción de valores no nulos en la columna
        print(column, c, p * 100)

dfclean

"""##codificacion de los datos"""

# Crear DataFrame para almacenar los datos codificados
df_encoded = dfclean.copy()
# Creamos un objeto LabelEncoder
label_encoder = LabelEncoder()

# Aplicamos label encoding a la columna 'make'
df_encoded['make_encoded'] = label_encoder.fit_transform(dfclean['make'])
df_encoded['body-style_encoded'] = label_encoder.fit_transform(dfclean['body-style'])
df_encoded['drive-wheels_encoded'] = label_encoder.fit_transform(dfclean['drive-wheels'])
df_encoded['engine-type_encoded'] = label_encoder.fit_transform(dfclean['engine-type'])
df_encoded['num-of-cylinders_encoded'] = label_encoder.fit_transform(dfclean['num-of-cylinders'])
df_encoded['fuel-type_encoded'] = label_encoder.fit_transform(dfclean['fuel-type'])
df_encoded['num-of-doors_encoded'] = label_encoder.fit_transform(dfclean['num-of-doors'])
df_encoded['aspiration_encoded'] = label_encoder.fit_transform(dfclean['aspiration'])
df_encoded['engine-location_encoded'] = label_encoder.fit_transform(dfclean['aspiration'])

# Visualizamos el DataFrame con la variable categórica codificada
df_encoded

drop_columns = ['make', 'body-style', 'drive-wheels', 'engine-type','fuel-type', 'num-of-cylinders','aspiration','num-of-doors','engine-location']
df_encoded.drop(drop_columns, axis=1, inplace=True)
df_encoded

"""## Arboles de decicion"""

# 1. Divide tus datos en características (X) y la variable objetivo (y)
X = df_encoded.drop('symboling', axis=1)
y = df_encoded['symboling']

# 2. Divide tus datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Inicializa el RandomForestClassifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# 4. Entrena el modelo
rf_classifier.fit(X_train, y_train)

# 5. Evalúa el modelo
y_pred = rf_classifier.predict(X_test)

# Métricas de evaluación
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

# Importacion de las librerias
import pandas as pd
from google.colab import files
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score

#asignacion de los nombres de las columnas
datos = [
    "symboling", "normalized-losses", "make", "fuel-type", "aspiration", "num-of-doors", "body-style", "drive-wheels","engine-location", "wheel-base", "length", "width", "height", "curb-weight", "engine-type", "num-of-cylinders", "engine-size",    "fuel-system", "bore", "stroke", "compression-ratio", "horsepower", "peak-rpm", "city-mpg", "highway-mpg", "price"
]

#importacion del dataframe
df = pd.read_csv("/content/drive/MyDrive/inteligencia computacional/Trabajo final del diplomado/archivos/imports-85.data", names=datos, na_values="?")

#Visualizacion del dataframe
df

"""##Preparacion de los datos"""

# Cargar el conjunto de datos
data = df

# Análisis de correlación
correlation = data[['symboling', 'normalized-losses']].corr()

# Visualización
sns.scatterplot(x='symboling', y='normalized-losses', data=data)
plt.title('Relación entre Symboling y Normalized-losses')
plt.xlabel('Symboling')
plt.ylabel('Normalized-losses')
plt.show()

# Coeficiente de correlación
print("Coeficiente de correlación:")
print(correlation)

# Visualización de la distribución de "normalized-losses" utilizando un histograma
plt.figure(figsize=(8, 6))
sns.histplot(df['normalized-losses'], bins=20, kde=False, color='skyblue')
plt.title('Distribución de normalized-losses')
plt.xlabel('normalized-losses')
plt.ylabel('Frecuencia')
plt.show()

# Visualización de la distribución de "symboling" utilizando un histograma
plt.figure(figsize=(8, 6))
sns.histplot(df['symboling'], bins=6, kde=False, color='skyblue')
plt.title('Distribución de Symboling')
plt.xlabel('Symboling')
plt.ylabel('Frecuencia')
plt.show()

# Exploración de la relación entre "symboling" y "price" utilizando un gráfico de dispersión
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='symboling', y='price', color='salmon')
plt.title('Relación entre Symboling y Price')
plt.xlabel('Symboling')
plt.ylabel('Precio')
plt.show()

# Exploración de la relación entre "symboling" y "make" utilizando un boxplot
plt.figure(figsize=(12, 8))
sns.boxplot(data=df, x='make', y='symboling', palette='viridis')
plt.title('Relación entre Symboling y Make')
plt.xlabel('Marca')
plt.ylabel('symboling')
plt.xticks(rotation=90)
plt.show()

# Estadísticas descriptivas de "symboling"
print("Estadísticas descriptivas de Symboling:")
print(df['symboling'].describe())

# Calcular la media de Symboling
media = df['symboling'].mean()

# Calcular la mediana de Symboling
mediana = df['symboling'].median()

# Calcular la moda de Symboling
moda = df['symboling'].mode()[0]  # Puede haber más de una moda, por lo que se toma la primera (si existe)

# Calcular la desviación estándar de Symboling
desviacion_estandar = df['symboling'].std()

# Imprimir los resultados
print("Media de Symboling:", media)
print("Mediana de Symboling:", mediana)
print("Moda de Symboling:", moda)
print("Desviación Estándar de Symboling:", desviacion_estandar)

# Calcular la media de normalized-losses
media = df['normalized-losses'].mean()

# Calcular la mediana de normalized-losses
mediana = df['normalized-losses'].median()

# Calcular la moda de normalized-losses
moda = df['normalized-losses'].mode()[0]  # Puede haber más de una moda, por lo que se toma la primera (si existe)

# Calcular la desviación estándar de normalized-losses
desviacion_estandar = df['normalized-losses'].std()

# Imprimir los resultados
print("Media de normalized-losses:", media)
print("Mediana de normalized-losses:", mediana)
print("Moda de normalized-losses:", moda)
print("Desviación Estándar de normalized-losses:", desviacion_estandar)

"""##comparacion la realcion entre normalized-losses"""

# Visualización de la distribución de "normalized-losses" utilizando un histograma
plt.figure(figsize=(8, 6))
sns.histplot(df['normalized-losses'], bins=20, kde=False, color='skyblue')
plt.title('Distribución de normalized-losses')
plt.xlabel('normalized-losses')
plt.ylabel('Frecuencia')
plt.show()

# Visualización de la distribución de "symboling" utilizando un histograma
plt.figure(figsize=(8, 6))
sns.histplot(df['normalized-losses'], bins=6, kde=False, color='skyblue')
plt.title('Distribución de normalized-losses')
plt.xlabel('normalized-losses')
plt.ylabel('Frecuencia')
plt.show()

# Exploración de la relación entre "symboling" y "price" utilizando un gráfico de dispersión
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='normalized-losses', y='price', color='salmon')
plt.title('Relación entre normalized-losses y Price')
plt.xlabel('normalized-losses')
plt.ylabel('Precio')
plt.show()

# Exploración de la relación entre "symboling" y "make" utilizando un boxplot
plt.figure(figsize=(12, 8))
sns.boxplot(data=df, x='make', y='normalized-losses', palette='viridis')
plt.title('Relación entre normalized-losses y marca')
plt.xlabel('Marca')
plt.ylabel('normalized-losses')
plt.xticks(rotation=90)
plt.show()

# Estadísticas descriptivas de "symboling"
print("Estadísticas descriptivas de Symboling:")
print(df['normalized-losses'].describe())

"""## Analisis de valores nulos"""

N = len(df)  # Obtener el número total de filas en el DataFrame

for column in df:
    if column not in ["normalized-losses", "num-of-doors",'bore', 'stroke', 'horsepower', 'peak-rpm', 'price']:
        print(column)
        c = df[column].isnull().sum()  # Contar el número de valores nulos en la columna
        p = 1 - c / N  # Calcular la proporción de valores no nulos en la columna
        print(column, c, p * 100)
    else:
        print(column)
        c = df[column].isnull().sum()  # Contar el número de valores nulos en la columna
        p = 1 - c / N  # Calcular la proporción de valores no nulos en la columna
        print(column, c, p * 100)

# Hacer una copia del DataFrame
dfclean = df.copy()

# Crear un imputador para reemplazar valores faltantes con la media
imputer_num = SimpleImputer(strategy='mean')

# Seleccionar columnas numéricas con datos faltantes
data_mising = ['normalized-losses', 'bore', 'stroke', 'peak-rpm', 'horsepower', 'price']

# Imputar valores faltantes en columnas numéricas
dfclean[data_mising] = imputer_num.fit_transform(dfclean[data_mising])

# Imputación de datos faltantes para variables categóricas
# Crear un imputador para reemplazar valores faltantes con el valor más frecuente
cat_imp = SimpleImputer(strategy='most_frequent')

# Seleccionar columnas categóricas con datos faltantes
cat_mising = ['num-of-doors']

# Imputar valores faltantes en columnas categóricas
dfclean[cat_mising] = cat_imp.fit_transform(dfclean[cat_mising])

columns_to_drop = ['fuel-system', 'bore', 'stroke', 'peak-rpm']
dfclean.drop(columns_to_drop, axis=1, inplace=True)

N = len(dfclean)  # Obtener el número total de filas en el DataFrame

for column in dfclean:
    if column not in ["normalized-losses", "num-of-doors",'bore', 'stroke', 'horsepower', 'peak-rpm', 'price']:
        print(column)
        c = dfclean[column].isnull().sum()  # Contar el número de valores nulos en la columna
        p = 1 - c / N  # Calcular la proporción de valores no nulos en la columna
        print(column, c, p * 100)
    else:
        print(column)
        c = dfclean[column].isnull().sum()  # Contar el número de valores nulos en la columna
        p = 1 - c / N  # Calcular la proporción de valores no nulos en la columna
        print(column, c, p * 100)

dfclean

"""##codificacion de los datos"""

# Crear DataFrame para almacenar los datos codificados
df_encoded = dfclean.copy()
# Creamos un objeto LabelEncoder
label_encoder = LabelEncoder()

# Aplicamos label encoding a la columna 'make'
df_encoded['make_encoded'] = label_encoder.fit_transform(dfclean['make'])
df_encoded['body-style_encoded'] = label_encoder.fit_transform(dfclean['body-style'])
df_encoded['drive-wheels_encoded'] = label_encoder.fit_transform(dfclean['drive-wheels'])
df_encoded['engine-type_encoded'] = label_encoder.fit_transform(dfclean['engine-type'])
df_encoded['num-of-cylinders_encoded'] = label_encoder.fit_transform(dfclean['num-of-cylinders'])
df_encoded['fuel-type_encoded'] = label_encoder.fit_transform(dfclean['fuel-type'])
df_encoded['num-of-doors_encoded'] = label_encoder.fit_transform(dfclean['num-of-doors'])
df_encoded['aspiration_encoded'] = label_encoder.fit_transform(dfclean['aspiration'])
df_encoded['engine-location_encoded'] = label_encoder.fit_transform(dfclean['aspiration'])

# Visualizamos el DataFrame con la variable categórica codificada
df_encoded

drop_columns = ['make', 'body-style', 'drive-wheels', 'engine-type','fuel-type', 'num-of-cylinders','aspiration','num-of-doors','engine-location']
df_encoded.drop(drop_columns, axis=1, inplace=True)
df_encoded

"""## Arboles de decicion"""

# 1. Divide tus datos en características (X) y la variable objetivo (y)
X = df_encoded.drop('symboling', axis=1)
y = df_encoded['symboling']

# 2. Divide tus datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Inicializa el RandomForestClassifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# 4. Entrena el modelo
rf_classifier.fit(X_train, y_train)

# 5. Evalúa el modelo
y_pred = rf_classifier.predict(X_test)

# Métricas de evaluación
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

df_encoded.to_csv('/content/drive/MyDrive/df_encoded.csv', index=False)

import joblib

# Guardar el modelo
joblib.dump(rf_classifier, 'modelo_rf.pkl')

from google.colab import files

# Descargar el modelo
files.download('modelo_rf.pkl')

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Supongamos que tienes tus datos X (características) y y (etiquetas)
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Escalado de características para mejorar el rendimiento del modelo
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Inicialización y entrenamiento del modelo
model = LogisticRegression()
model.fit(X_train_scaled, y_train)

# Predicciones en el conjunto de prueba
y_pred = model.predict(X_test_scaled)

# Evaluación del modelo
accuracy = accuracy_score(y_test, y_pred)
print("Exactitud del modelo:",accuracy)